{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Murcha1990/ML_AI25/blob/main/Hometasks/Base/AI_HW3_Classification_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "125eAde1VB9p"
   },
   "source": [
    "# **Домашнее задание 3. Линейная классификация (base)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mza-ytI_VB9t"
   },
   "source": [
    "### **Оценивание и штрафы**\n",
    "\n",
    "С наступающим новым годом, друзья! Магистратура бежит быстро и мы бежим очень быстро, а зима — то время, когда хотелось бы бежать чуть медленнее. Поэтому это домашнее задание мы сделали сильно короче от его начальной версии!\n",
    "\n",
    "Как всегда, каждая из задач имеет «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов.\n",
    "\n",
    "В задании две части:\n",
    "\n",
    "- Часть 1: написание логистической регрессии своими руками — уверенны, логлосс вы уже знаете как свои пять пальцев.\n",
    "- Часть 2: решение задачи классификации на текстах."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QQo0z9ZGVB9v",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bfca6a5c-f33e-4fc4-8696-33c3fea3271c",
    "ExecuteTime": {
     "end_time": "2025-12-31T12:50:14.689432Z",
     "start_time": "2025-12-31T12:50:14.685189Z"
    }
   },
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "#from google.colab import drive # Если вы работаете в коллабе\n",
    "#drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rd6oOpNSfdGH",
    "outputId": "dff08809-bc72-41cb-dfc2-4f00b1a8f84e",
    "ExecuteTime": {
     "end_time": "2025-12-31T12:52:36.140537Z",
     "start_time": "2025-12-31T12:52:36.138511Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9L36HLHVB9w"
   },
   "source": [
    "# **Часть 1. Логистическая регрессия своими руками (5 баллов)**\n",
    "\n",
    "Логистическая регрессия — безумно важная и удобная модель для понимания начальных концепций. Вы много практиковались с выведением формулы градиента логлосса, шага спуска, а в прошлом дз сделали SGD. Давайте сделаем ещё шаг вперед — и реализуем логистическую регрессию своими руками.\n",
    "\n",
    "На практике, часто хватает алгоритмов из коробки. Но иногда очень удобно сделать свой алгоритм."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Теоретическая сноска: почему LogLoss'а так много**\n",
    "\n",
    "Почти наверное (в математике это значит, во всех случаях, кроме множества размером 0) логлосс набил оскомину за несколько заданий. Давайте посмотрим на него ещё раз:\n",
    "\n",
    "В логистической регрессии функция потерь\n",
    "\n",
    "$$\\text{LogLoss}(y, \\hat{p}) = -\\left(y\\log \\hat{p} + (1-y)\\log (1-\\hat{p})\\right)$$\n",
    "\n",
    "Зачем мы так долго с ней возимся?\n",
    "\n",
    "#### **Пункт 1.**\n",
    "Во-первых, это **следствие максимизации правдоподобия** при биномиальной модели.\n",
    "\n",
    "Если считать, что целевая переменная (Y\\in{0,1}) распределена как\n",
    "\n",
    "$$P(Y=1 \\mid x) = \\hat{p}(x), \\qquad P(Y=0\\mid x) = 1-\\hat{p}(x),$$\n",
    "то правдоподобие выборки ( (x_i, y_i) )\\ равно\n",
    "$$L = \\prod_{i=1}^n \\hat{p}_i^{y_i}(1-\\hat{p}_i)^{1-y_i}.$$\n",
    "\n",
    "Максимизация $\\log L$ эквивалентна минимизации LogLoss.\n",
    "Таким образом, LogLoss — **единственная функция потерь, полностью согласованная с вероятностной моделью логистической регрессии**.\n",
    "\n",
    "#### **Пункт 2.**\n",
    "\n",
    "Во-вторых, логлосс поможет нам в будущем понять другие функции потерь. Так, например LogLoss является частным случаем **кросс-энтропии между истинным распределением и предсказанным**.\n",
    "\n",
    "Для двух распределений $p$ (истинного) и $q$ (предсказанного) кросс-энтропия определяется как\n",
    "\n",
    "$$H(p,q) = -\\sum_{k} p(k)\\log q(k).$$\n",
    "\n",
    "В бинарном случае истинное распределение дискретно:\n",
    "\n",
    "$$p = (y, 1-y), \\qquad q = (\\hat{p}, 1-\\hat{p}),$$\n",
    "и подстановка даёт\n",
    "\n",
    "$$H(p,q) = -\\left[y\\log \\hat{p} + (1-y)\\log (1-\\hat{p})\\right] = \\text{LogLoss}.$$\n",
    "\n",
    "\n",
    "В общем, любим, жалуем и реализуем.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "B2iCqkOMy-6t"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T07:36:04.765536Z",
     "start_time": "2018-10-12T07:35:57.814973Z"
    },
    "id": "_rilRoZZVB9w"
   },
   "source": [
    "### **Задание 1. Реализуйте класс логистической регрессии, обучаемой с помощью:**\n",
    "\n",
    "**Задание 1.1 (1.5 балла). Градиентного спуска**\n",
    "\n",
    "**Задание 1.2 (1.5 балла). Стохастического градиентного спуска**\n",
    "\n",
    "До этого вы писали код без ограничений. Здесь же необходимо соблюдать следующие условия:\n",
    "\n",
    "- Градиентный спуск необходимо записать в векторном виде;\n",
    "- Циклы средствами python допускается использовать только для итераций градиентного спуска;\n",
    "\n",
    "**Класс градиентного спуска должен:**\n",
    "- В качестве критерия останова использовать (одновременно):\n",
    "  - проверку на евклидову норму разности весов на двух соседних итерациях задаваемого параметром `tolerance`;\n",
    "  - достижение максимального числа итераций, задаваемого параметром `max_iter`.\n",
    "- Обладать атрибутом `loss_history`. В нём после вызова метода fit должны содержаться значения функции потерь для всех итераций, начиная с первой (до совершения первого шага по антиградиенту). Данный атрибут необходим, чтобы проследить, что оптимизационный процесс действительно сходится;\n",
    "- Инициализировать веса случайным образом или нулевым вектором (на ваш выбор)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0hcxIOiVB9w"
   },
   "source": [
    "Полезно [почитать](https://scikit-learn.org/stable/developers/develop.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Шаблон класса описан ниже, вам нужно реализовать каждую из заготовленных функций.**\n",
    "\n",
    "**ВАЖНО!** Мы заполняем данный шаблон, даже если он нам не нравится. Менять структуру класса и писать по-своему запрещено - за это будут сняты баллы."
   ],
   "metadata": {
    "id": "SgzMXEhzXEkI"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jPtVGuYxVB9w",
    "ExecuteTime": {
     "end_time": "2025-12-31T13:38:16.833353Z",
     "start_time": "2025-12-31T13:38:16.791519Z"
    }
   },
   "source": "import numpy as np\nfrom sklearn.base import BaseEstimator\n\nclass LogReg(BaseEstimator):\n    def __init__(self, gd_type='stochastic',\n                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2):\n        \"\"\"\n        gd_type: 'full' or 'stochastic'\n        tolerance: for stopping gradient descent\n        max_iter: maximum number of steps in gradient descent\n        w0: np.array of shape (d) — init weights\n        eta: learning rate\n        \"\"\"\n        self.gd_type = gd_type\n        self.tolerance = tolerance\n        self.max_iter = max_iter\n        self.w0 = w0\n        self.w = None\n        self.eta = eta\n        self.loss_history = []\n\n    def fit(self, X, y):\n        \"\"\"\n        X: np.array of shape (ell, d)\n        y: np.array of shape (ell)\n        ---\n        output: self\n        \"\"\"\n        ell, d = X.shape\n\n        # Инициализация весов\n        if self.w0 is not None:\n            self.w = self.w0.copy()\n        else:\n            self.w = np.zeros(d)\n\n        self.loss_history = []\n\n        for _ in range(self.max_iter):\n            # Записываем loss до обновления весов\n            self.loss_history.append(self.calc_loss(X, y))\n\n            # Вычисляем градиент\n            if self.gd_type == 'full':\n                grad = self.calc_gradient(X, y)\n            else:  # стохастический градиент\n                idx = np.random.randint(0, ell)\n                grad = self.calc_gradient(X[idx:idx+1], y[idx:idx+1])\n\n            # Обновляем веса\n            w_new = self.w - self.eta * grad\n\n            # Проверка критерия останова по норме разности весов\n            if np.linalg.norm(w_new - self.w) < self.tolerance:\n                self.w = w_new\n                break\n            self.w = w_new\n\n        return self\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n\n    def predict_proba(self, X):\n        if self.w is None:\n            raise Exception('Not trained yet')\n        return self.sigmoid(X @ self.w)\n\n    def predict(self, X):\n        if self.w is None:\n            raise Exception('Not trained yet')\n        return (self.predict_proba(X) >= 0.5).astype(int)\n\n    def calc_gradient(self, X, y):\n        \"\"\"\n        X: np.array of shape (ell, d) (ell can be equal to 1 if stochastic)\n        y: np.array of shape (ell)\n        ---\n        output: np.array of shape (d)\n        \"\"\"\n        # Градиент LogLoss в векторном виде: (1/ell) * X^T * (sigmoid(Xw) - y)\n        ell = X.shape[0]\n        p = self.sigmoid(X @ self.w)\n        grad = (X.T @ (p - y)) / ell\n        return grad\n\n    def calc_loss(self, X, y):\n        \"\"\"\n        X: np.array of shape (ell, d)\n        y: np.array of shape (ell)\n        ---\n        output: float\n        \"\"\"\n        eps = 1e-15\n        p = self.sigmoid(X @ self.w)\n        loss = -np.mean(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))\n        return loss",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь проверим работу вашего класса на синтетических данных."
   ],
   "metadata": {
    "id": "T5IcgSNW4bUp"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hOnxyTS7VB9y",
    "ExecuteTime": {
     "end_time": "2025-12-31T13:38:27.715341Z",
     "start_time": "2025-12-31T13:38:27.609262Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=100000, n_features=20, n_informative=2,\n",
    "    random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2b-BcRdVB9y"
   },
   "source": [
    "### **Задание 2 (0.6 балла)**\n",
    "\n",
    "Теперь давайте тестировать модель.\n",
    "1. Обучите свою логистическую регрессию на синтетических данных (0.2 балла) — на полном GD и SGD;\n",
    "2. Cравните результат с моделью из библиотеки. Посчитайте roc-auc, accuracy, постройте ROC и PR кривые. , оцените разницу в производительности моделей по метрикам качества. Ответьте на вопросы:\n",
    "- Какая показывает лучший результат? Почему?\n",
    "- Есть ли что-то в модели из коробки, что по умолчанию делает её не равной вашей модели? Для ответа на этот вопрос вам может пригодитться [документация](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Её мы изучаем всегда, чтобы понимать тонкости реализации какого-либо метода в библиотеке. (0.4 балла)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "lr_sgd = LogReg(gd_type='stochastic')\n",
    "lr_sgd.fit(X_train, y_train)\n",
    "\n",
    "lr_full = LogReg(gd_type='full')\n",
    "lr_full.fit(X_train, y_train)\n"
   ],
   "metadata": {
    "id": "4L4x092dLh-g"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T20:39:43.088969Z",
     "start_time": "2018-10-11T20:39:43.084985Z"
    },
    "id": "xZ2whMm3VB9y"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Обучаем логистическую регрессию на синтетических данных\n",
    "lr_sklearn = LogisticRegression(max_iter=1000)\n",
    "lr_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# Получаем предсказания\n",
    "models = {\n",
    "    'SGD (custom)': lr_sgd,\n",
    "    'Full GD (custom)': lr_full,\n",
    "    'sklearn LogReg': lr_sklearn,\n",
    "}\n",
    "\n",
    "# Считаем метрики\n",
    "print('='*60)\n",
    "print(f'{'Модель':<20}' {'Accuracy':<12} {'ROC-AUC':<12})\n",
    "print('='*60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    #sklearn возращает 2D массив для predict_proba\n",
    "    if y_proba.ndim == 2:\n",
    "        y_proba = y_proba[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    roc = roc_auc_score(y_test, y_proba)\n",
    "    print(f'{name:<20} {acc:<12.4f} {roc:<12.4f}')\n",
    "\n",
    "print('='*60)\n",
    "\n",
    "#Построение ROC кривых\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "for name, model in models.items():\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    if y_proba.ndim == 2:\n",
    "        y_proba = y_proba[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    ax2.plot(recall, precision, label = f'{name} (AUC = {pr_auc:.4f})')\n",
    "\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Precision-Recall curve')\n",
    "ax2.legend(loc = 'lower right')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Задание 3 (0.4 балла)**\n",
    "\n",
    "Для трех полученных моделей, визуализируйте прогнозы по данным на тестовой выборке. Для этого:\n",
    "- получите прогнозы;\n",
    "- сомжите данные, используя PCA. Не забудьте, что PCA полагает нулевое среднее и единичную дисперсию;\n",
    "- покрасьте данные по прогнозам.\n",
    "\n",
    "Как различаются графики для трёх моделей? И различаются ли?"
   ],
   "metadata": {
    "id": "Cp7jXN3lv_u7"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "c0AcJrf_vpVb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZ0ZI7v1VB9z"
   },
   "source": [
    "# **Часть 2. Обучение моделей на текстовых данных. (5 баллов)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T20:41:54.913436Z",
     "start_time": "2018-10-11T20:41:54.907515Z"
    },
    "id": "KBWjsPSSVB9z"
   },
   "source": [
    " ### **Подготовка данных из реального мира.**\n",
    "\n",
    "Ещё одна прелесть простых моделей — возможность решать с ними неструктурированные (изначально не табличные) задачи. Давайте посмотрим на это в действии на примере текстов.\n",
    "\n",
    "\n",
    "Загрузите данные с конкурса  [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started/data?select=train.csv) (вам нужна только обучающая выборка, файл `train.csv`). Задача состоит в определении постов, сообщающих о чрезвычайной ситуации. В рамках домашнего задания, этот набор данных будет отличным полем для тренировки в обработке признаков."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UF_dt9lcVB90",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "outputId": "16866bee-359d-47a7-d429-60b90d57edf3",
    "ExecuteTime": {
     "end_time": "2025-12-31T14:56:22.719115Z",
     "start_time": "2025-12-31T14:56:21.114039Z"
    }
   },
   "source": "from sklearn.model_selection import train_test_split\n\nPATH = 'train.csv'\ndata = pd.read_csv(PATH)\n\ndata.head()",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[1;32m      3\u001B[0m PATH \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain.csv\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 4\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mread_csv(PATH)\n\u001B[1;32m      6\u001B[0m data\u001B[38;5;241m.\u001B[39mhead()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Задание 10. Базовая предобработка (1.5 балла).**\n",
    "\n",
    "- Выведите на экран информацию о пропусках в данных. Если пропуски присутствуют заполните их пустой строкой (0.2 балла)."
   ],
   "metadata": {
    "id": "3REJEAhnPEAq"
   }
  },
  {
   "cell_type": "code",
   "source": "# Анализ пропусков\nprint(\"Информация о пропусках в данных:\")\nprint(data.isnull().sum())\nprint(\"\\nПроцент пропусков:\")\nprint(data.isnull().sum() / len(data) * 100)\n\n# Заполнение пропусков пустой строкой\ndata['keyword'] = data['keyword'].fillna('')\ndata['location'] = data['location'].fillna('')\ndata['text'] = data['text'].fillna('')\n\nprint(\"\\nПосле заполнения пропусков:\")\nprint(data.isnull().sum())",
   "metadata": {
    "id": "x9AF0Ns6PPxO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Проанализируйте количество уникальных значений в столбцах, опустив `text`. Сделайте выводы. (0.5 балла)"
   ],
   "metadata": {
    "id": "Nh1tdl-EPorP"
   }
  },
  {
   "cell_type": "code",
   "source": "# Анализ уникальных значений (без столбца text)\nprint(\"Количество уникальных значений в столбцах:\")\nfor col in data.columns:\n    if col != 'text':\n        print(f\"{col}: {data[col].nunique()}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ВЫВОДЫ:\")\nprint(\"=\"*60)\nprint(\"1. id: {} уникальных значений - это идентификаторы твитов\".format(data['id'].nunique()))\nprint(\"2. keyword: {} уникальных значений - ключевые слова из твитов\".format(data['keyword'].nunique()))\nprint(\"   (с учетом пустых строк после заполнения пропусков)\")\nprint(\"3. location: {} уникальных значений - местоположения авторов\".format(data['location'].nunique()))\nprint(\"   (очень много разных локаций, что может указывать на шум в данных)\")\nprint(\"4. target: {} класса - бинарная классификация (0 - не ЧС, 1 - ЧС)\".format(data['target'].nunique()))",
   "metadata": {
    "id": "uLjxpt7bPnST",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Проанализируйте соотношение классов в целевой переменной. Какое оно? Выберите метрику, с помощью которой будете оценивать модель.  (0.5 балла)"
   ],
   "metadata": {
    "id": "U0C59BsSQU3H"
   }
  },
  {
   "cell_type": "code",
   "source": "# Анализ баланса классов\nprint(\"Распределение классов:\")\nprint(data['target'].value_counts())\nprint(\"\\nПроцентное соотношение:\")\nprint(data['target'].value_counts(normalize=True) * 100)\n\n# Визуализация\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(8, 5))\ndata['target'].value_counts().plot(kind='bar', ax=ax)\nax.set_xlabel('Класс (0 - не ЧС, 1 - ЧС)')\nax.set_ylabel('Количество')\nax.set_title('Распределение классов в целевой переменной')\nax.set_xticklabels(['Не ЧС (0)', 'ЧС (1)'], rotation=0)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ВЫВОД:\")\nprint(\"=\"*60)\nprint(\"Соотношение классов примерно 57% : 43% (не ЧС : ЧС).\")\nprint(\"Данные слегка несбалансированы, но не критично.\")\nprint(\"\\nВыбранная метрика: ROC-AUC\")\nprint(\"Причина: ROC-AUC хорошо работает при небольшом дисбалансе классов\")\nprint(\"и оценивает способность модели различать классы по вероятностям.\")\nprint(\"Также будем использовать accuracy для дополнительной оценки.\")",
   "metadata": {
    "id": "STjfzgq9Qecn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Объедините все три текстовых столбца в один для baseline (вам поможет конкатенация строк) (0.3 балла)"
   ],
   "metadata": {
    "id": "A7xePrWCPfEm"
   }
  },
  {
   "cell_type": "code",
   "source": "# Объединение текстовых столбцов для baseline\n# Конкатенация keyword, location и text с пробелами\ndata_new = data.copy()\ndata_new['text'] = data['keyword'] + ' ' + data['location'] + ' ' + data['text']\n\nprint(\"Пример объединенного текста:\")\nprint(data_new['text'].head(3))\nprint(\"\\nФорма данных:\", data_new.shape)",
   "metadata": {
    "id": "ESR7etPOQw0m"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Наконец, поделите данные на тренировочную и тестовую выборки. (0.2 балла)"
   ],
   "metadata": {
    "id": "syMOkjGcSA3z"
   }
  },
  {
   "cell_type": "code",
   "source": "X = data_new['text']\ny = data['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint(\"Размер обучающей выборки:\", X_train.shape)\nprint(\"Размер тестовой выборки:\", X_test.shape)\nprint(\"Распределение классов в обучающей выборке:\")\nprint(y_train.value_counts())",
   "metadata": {
    "id": "pRuY5gu2O00y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxmJha91VB90"
   },
   "source": [
    "### **Задание 11. Базовые модели. (1 балл).**\n",
    "\n",
    "Данные, собираемые с сайтов, часто содержат мусор не информативный для моделей. Посмотрите, какого качества и насколько разнообразны данные здесь. Для этого:\n",
    "- Примените CountVectorizer из sklearn к сырым даным. Какого размера получилась матрица? (0.3 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcWyCKFFVB90"
   },
   "outputs": [],
   "source": "from sklearn.feature_extraction.text import CountVectorizer\n\n# Применяем CountVectorizer к сырым данным\nvectorizer = CountVectorizer()\nX_train_vec = vectorizer.fit_transform(X_train)\nX_test_vec = vectorizer.transform(X_test)\n\nprint(\"Размер матрицы признаков (обучающая выборка):\", X_train_vec.shape)\nprint(\"Размер матрицы признаков (тестовая выборка):\", X_test_vec.shape)\nprint(\"\\nКоличество объектов:\", X_train_vec.shape[0])\nprint(\"Количество признаков (размер словаря):\", X_train_vec.shape[1])\nprint(\"\\nМатрица получилась очень разреженной с {} уникальными словами\".format(X_train_vec.shape[1]))"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Обучите логистическую регрессию на полученном наборе. Модель возьмите из библиотеки. Какое качество по выбранной вами метрике у модели получилось на тестовых данных? (0.3 балла)"
   ],
   "metadata": {
    "id": "i6qBosrkWO1M"
   }
  },
  {
   "cell_type": "code",
   "source": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport time\n\n# Обучаем логистическую регрессию\nlr = LogisticRegression(max_iter=1000, random_state=42)\n\nstart_time = time.time()\nlr.fit(X_train_vec, y_train)\ntrain_time = time.time() - start_time\n\n# Предсказания\ny_pred_lr = lr.predict(X_test_vec)\ny_proba_lr = lr.predict_proba(X_test_vec)[:, 1]\n\n# Метрики\nacc_lr = accuracy_score(y_test, y_pred_lr)\nroc_auc_lr = roc_auc_score(y_test, y_proba_lr)\n\nprint(\"=\"*60)\nprint(\"Логистическая регрессия (baseline)\")\nprint(\"=\"*60)\nprint(f\"Время обучения: {train_time:.2f} секунд\")\nprint(f\"Accuracy на тесте: {acc_lr:.4f}\")\nprint(f\"ROC-AUC на тесте: {roc_auc_lr:.4f}\")\nprint(\"=\"*60)",
   "metadata": {
    "id": "mLacyuMLV5DR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Визуализация ROC-кривой для логистической регрессии\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n\nfpr, tpr, _ = roc_curve(y_test, y_proba_lr)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {roc_auc_lr:.4f})')\nplt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve - Logistic Regression')\nplt.legend()\nplt.grid(True)\nplt.show()",
   "metadata": {
    "id": "AqRVll2GWZYH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Обучите SVC на тех же данных с гиперпараметрами по умолчанию. Измерьте качество на тестовых данных и опишите результат. Проанализируйте качество и скорость обучения.(0.4 балла)"
   ],
   "metadata": {
    "id": "v7DrpPhVaHUP"
   }
  },
  {
   "cell_type": "code",
   "source": "from sklearn.svm import SVC\nimport time\n\n# Обучаем SVC с гиперпараметрами по умолчанию\nsvc = SVC(random_state=42, probability=True)\n\nstart_time = time.time()\nsvc.fit(X_train_vec, y_train)\ntrain_time_svc = time.time() - start_time\n\n# Предсказания\ny_pred_svc = svc.predict(X_test_vec)\ny_proba_svc = svc.predict_proba(X_test_vec)[:, 1]\n\n# Метрики\nacc_svc = accuracy_score(y_test, y_pred_svc)\nroc_auc_svc = roc_auc_score(y_test, y_proba_svc)\n\nprint(\"=\"*60)\nprint(\"SVC (baseline)\")\nprint(\"=\"*60)\nprint(f\"Время обучения: {train_time_svc:.2f} секунд\")\nprint(f\"Accuracy на тесте: {acc_svc:.4f}\")\nprint(f\"ROC-AUC на тесте: {roc_auc_svc:.4f}\")\nprint(\"=\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"АНАЛИЗ РЕЗУЛЬТАТОВ:\")\nprint(\"=\"*60)\nprint(f\"Логистическая регрессия:\")\nprint(f\"  - Время обучения: {train_time:.2f} сек\")\nprint(f\"  - ROC-AUC: {roc_auc_lr:.4f}\")\nprint(f\"\\nSVC:\")\nprint(f\"  - Время обучения: {train_time_svc:.2f} сек\")\nprint(f\"  - ROC-AUC: {roc_auc_svc:.4f}\")\nprint(f\"\\nВЫВОД:\")\nprint(f\"SVC обучается значительно медленнее (в {train_time_svc/train_time:.1f} раз),\")\nprint(f\"но показывает {'лучшее' if roc_auc_svc > roc_auc_lr else 'аналогичное'} качество.\")\nprint(\"Для больших датасетов логистическая регрессия предпочтительнее\")\nprint(\"из-за скорости обучения при сопоставимом качестве.\")",
   "metadata": {
    "id": "9rpSS46UadkB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Задание 12. Улучшение базовых моделей за счет данных. (0.3 балла).**"
   ],
   "metadata": {
    "id": "01Pjk6sVawv_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Подберите гиперпараметры CountVectorizer так, чтобы признаков было минимум в 4 раза меньше, чем объектов, а качество модели при этом изменилось не более чем на $\\pm 0.07$. Опишите подобранные гиперпараметры и на что они влияют.\n",
    "\n",
    "Обучайте и логистическую регрессию, и SVC."
   ],
   "metadata": {
    "id": "7OwA5U3lVD2V"
   }
  },
  {
   "cell_type": "code",
   "source": "# Подбор гиперпараметров CountVectorizer\n# Цель: признаков минимум в 4 раза меньше объектов (5329 / 4 = 1332)\n# при этом качество должно измениться не более чем на ±0.07\n\n# Используем следующие гиперпараметры:\n# - max_features: ограничиваем количество признаков\n# - min_df: минимальная частота документов для слова\n# - max_df: максимальная частота документов (убираем очень частые слова)\n# - ngram_range: используем униграммы и биграммы\n\nvectorizer_improved = CountVectorizer(\n    max_features=1200,  # Ограничиваем количество признаков\n    min_df=2,           # Слово должно встречаться минимум в 2 документах\n    max_df=0.8,         # Убираем слова, встречающиеся более чем в 80% документов\n    ngram_range=(1, 2)  # Используем униграммы и биграммы\n)\n\nX_train_vec_improved = vectorizer_improved.fit_transform(X_train)\nX_test_vec_improved = vectorizer_improved.transform(X_test)\n\nprint(\"=\"*60)\nprint(\"Улучшенный CountVectorizer\")\nprint(\"=\"*60)\nprint(f\"Количество объектов: {X_train_vec_improved.shape[0]}\")\nprint(f\"Количество признаков: {X_train_vec_improved.shape[1]}\")\nprint(f\"Соотношение объекты/признаки: {X_train_vec_improved.shape[0] / X_train_vec_improved.shape[1]:.2f}\")\nprint(\"=\"*60)\n\nprint(\"\\nОписание подобранных гиперпараметров:\")\nprint(\"1. max_features=1200 - ограничивает словарь 1200 наиболее частыми словами\")\nprint(\"   Влияние: уменьшает размерность, убирает редкие/шумные признаки\")\nprint(\"\\n2. min_df=2 - слово должно встретиться минимум в 2 документах\")\nprint(\"   Влияние: фильтрует опечатки и очень редкие слова\")\nprint(\"\\n3. max_df=0.8 - исключает слова, встречающиеся в >80% документов\")\nprint(\"   Влияние: убирает стоп-слова (the, is, a, etc.)\")\nprint(\"\\n4. ngram_range=(1,2) - использует униграммы и биграммы\")\nprint(\"   Влияние: учитывает контекст слов (например, 'not good' vs 'good')\")",
   "metadata": {
    "id": "9SxcCaksUSSE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Обучаем LogisticRegression на улучшенных признаках\nlr_improved = LogisticRegression(max_iter=1000, random_state=42)\nlr_improved.fit(X_train_vec_improved, y_train)\n\ny_pred_lr_improved = lr_improved.predict(X_test_vec_improved)\ny_proba_lr_improved = lr_improved.predict_proba(X_test_vec_improved)[:, 1]\n\nacc_lr_improved = accuracy_score(y_test, y_pred_lr_improved)\nroc_auc_lr_improved = roc_auc_score(y_test, y_proba_lr_improved)\n\n# Обучаем SVC на улучшенных признаках\nsvc_improved = SVC(random_state=42, probability=True)\nsvc_improved.fit(X_train_vec_improved, y_train)\n\ny_pred_svc_improved = svc_improved.predict(X_test_vec_improved)\ny_proba_svc_improved = svc_improved.predict_proba(X_test_vec_improved)[:, 1]\n\nacc_svc_improved = accuracy_score(y_test, y_pred_svc_improved)\nroc_auc_svc_improved = roc_auc_score(y_test, y_proba_svc_improved)\n\n# Сравнение результатов\nprint(\"=\"*60)\nprint(\"СРАВНЕНИЕ РЕЗУЛЬТАТОВ\")\nprint(\"=\"*60)\nprint(f\"{'Модель':<30} {'Baseline ROC-AUC':<20} {'Improved ROC-AUC':<20} {'Изменение':<15}\")\nprint(\"-\"*60)\nprint(f\"{'Logistic Regression':<30} {roc_auc_lr:<20.4f} {roc_auc_lr_improved:<20.4f} {roc_auc_lr_improved - roc_auc_lr:+.4f}\")\nprint(f\"{'SVC':<30} {roc_auc_svc:<20.4f} {roc_auc_svc_improved:<20.4f} {roc_auc_svc_improved - roc_auc_svc:+.4f}\")\nprint(\"=\"*60)\n\n# Проверка условий задания\nchange_lr = abs(roc_auc_lr_improved - roc_auc_lr)\nchange_svc = abs(roc_auc_svc_improved - roc_auc_svc)\n\nprint(f\"\\nУсловие 1: Признаков в 4+ раза меньше объектов\")\nprint(f\"  {X_train_vec_improved.shape[0]} / {X_train_vec_improved.shape[1]} = {X_train_vec_improved.shape[0] / X_train_vec_improved.shape[1]:.2f} ✓\")\nprint(f\"\\nУсловие 2: Качество изменилось не более чем на ±0.07\")\nprint(f\"  LogReg: {change_lr:.4f} {'✓' if change_lr <= 0.07 else '✗'}\")\nprint(f\"  SVC: {change_svc:.4f} {'✓' if change_svc <= 0.07 else '✗'}\")",
   "metadata": {
    "id": "qqsV3rhWa7Br"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Задание 13. Улучшение базовых моделей за счет данных 2. (0.7 балла).**\n",
    "\n",
    "В первом пункте мы склеили все строки в одну. Но можно было бы поступить иначе — и получить категории из `'keyword', 'location'`. Протестируйте такой подход на обеих моделях и замерьте качество. Улучшает ли это результат?"
   ],
   "metadata": {
    "id": "5-9K39Hq2x2b"
   }
  },
  {
   "cell_type": "code",
   "source": "from sklearn.preprocessing import OneHotEncoder\nfrom scipy.sparse import hstack\nimport numpy as np\n\n# Вместо конкатенации, обрабатываем текст отдельно, а keyword и location как категории\n# Векторизуем только текст\nvectorizer_text = CountVectorizer(max_features=1200, min_df=2, max_df=0.8, ngram_range=(1, 2))\n\n# Подготовка данных\nX_train_text = data.loc[X_train.index, 'text'].fillna('')\nX_test_text = data.loc[X_test.index, 'text'].fillna('')\n\nX_train_text_vec = vectorizer_text.fit_transform(X_train_text)\nX_test_text_vec = vectorizer_text.transform(X_test_text)\n\n# Категориальные признаки: keyword и location\nX_train_keyword = data.loc[X_train.index, 'keyword'].fillna('').values.reshape(-1, 1)\nX_test_keyword = data.loc[X_test.index, 'keyword'].fillna('').values.reshape(-1, 1)\n\nX_train_location = data.loc[X_train.index, 'location'].fillna('').values.reshape(-1, 1)\nX_test_location = data.loc[X_test.index, 'location'].fillna('').values.reshape(-1, 1)\n\n# One-hot encoding для категорий\nencoder_keyword = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\nencoder_location = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\n\nX_train_keyword_enc = encoder_keyword.fit_transform(X_train_keyword)\nX_test_keyword_enc = encoder_keyword.transform(X_test_keyword)\n\nX_train_location_enc = encoder_location.fit_transform(X_train_location)\nX_test_location_enc = encoder_location.transform(X_test_location)\n\n# Объединяем все признаки\nX_train_combined = hstack([X_train_text_vec, X_train_keyword_enc, X_train_location_enc])\nX_test_combined = hstack([X_test_text_vec, X_test_keyword_enc, X_test_location_enc])\n\nprint(\"=\"*60)\nprint(\"Признаки с категориями\")\nprint(\"=\"*60)\nprint(f\"Размерность текстовых признаков: {X_train_text_vec.shape[1]}\")\nprint(f\"Размерность keyword признаков: {X_train_keyword_enc.shape[1]}\")\nprint(f\"Размерность location признаков: {X_train_location_enc.shape[1]}\")\nprint(f\"Итоговая размерность: {X_train_combined.shape[1]}\")\nprint(\"=\"*60)",
   "metadata": {
    "id": "T2imEAkd3BN2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Задание 13. Улучшение базовых моделей путем подбора гиперпараметров. (1 балл).**\n",
    "- Попробуйте подбирать разные гиперпараметры для логистической регрессии. Опишите подбираемые гиперапарметры и ваши результаты (0.5 балла)"
   ],
   "metadata": {
    "id": "AGTy5d7Vc-O9"
   }
  },
  {
   "cell_type": "code",
   "source": "from sklearn.model_selection import GridSearchCV\n\n# Подбор гиперпараметров для LogisticRegression\n# Основные гиперпараметры:\n# - C: обратная величина регуляризации (меньше C = сильнее регуляризация)\n# - penalty: тип регуляризации (l1, l2, elasticnet)\n# - solver: алгоритм оптимизации\n\nparam_grid_lr = {\n    'C': [0.1, 1, 10, 100],                    # Сила регуляризации\n    'penalty': ['l2'],                          # Тип регуляризации (l2 работает со всеми solvers)\n    'solver': ['lbfgs', 'liblinear', 'saga']   # Алгоритмы оптимизации\n}\n\nlr_grid = GridSearchCV(\n    LogisticRegression(max_iter=1000, random_state=42),\n    param_grid_lr,\n    cv=5,\n    scoring='roc_auc',\n    n_jobs=-1,\n    verbose=1\n)\n\nprint(\"Подбор гиперпараметров для LogisticRegression...\")\nlr_grid.fit(X_train_combined, y_train)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"РЕЗУЛЬТАТЫ ПОДБОРА ГИПЕРПАРАМЕТРОВ - LogisticRegression\")\nprint(\"=\"*60)\nprint(f\"Лучшие параметры: {lr_grid.best_params_}\")\nprint(f\"Лучший ROC-AUC (CV): {lr_grid.best_score_:.4f}\")\n\n# Тестируем на тестовой выборке\ny_proba_lr_tuned = lr_grid.best_estimator_.predict_proba(X_test_combined)[:, 1]\nroc_auc_lr_tuned = roc_auc_score(y_test, y_proba_lr_tuned)\nprint(f\"ROC-AUC на тесте: {roc_auc_lr_tuned:.4f}\")\nprint(f\"Улучшение по сравнению с baseline: {roc_auc_lr_tuned - roc_auc_lr:+.4f}\")\nprint(\"=\"*60)\n\nprint(\"\\nОПИСАНИЕ ГИПЕРПАРАМЕТРОВ:\")\nprint(\"1. C - обратная сила регуляризации:\")\nprint(\"   - Меньшие значения (0.1) = сильная регуляризация = проще модель\")\nprint(\"   - Большие значения (100) = слабая регуляризация = сложнее модель\")\nprint(\"\\n2. penalty - тип регуляризации:\")\nprint(\"   - 'l2' (Ridge): штрафует квадрат весов, делает их маленькими\")\nprint(\"   - 'l1' (Lasso): может обнулять веса, выполняет отбор признаков\")\nprint(\"\\n3. solver - алгоритм оптимизации:\")\nprint(\"   - 'lbfgs': эффективен для небольших датасетов, работает с l2\")\nprint(\"   - 'liblinear': хорош для малых данных, работает с l1 и l2\")\nprint(\"   - 'saga': быстрый на больших данных, работает со всеми penalty\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Ваш код здесь"
   ],
   "metadata": {
    "id": "EJNZOPy1dZQx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Подбор гиперпараметров для SVC\n# Основные гиперпараметры:\n# - C: параметр регуляризации\n# - kernel: тип ядра (linear, rbf, poly)\n# - gamma: коэффициент ядра для rbf, poly\n\n# Используем меньший grid для SVC, так как он обучается медленно\nparam_grid_svc = {\n    'C': [0.1, 1, 10],                    # Сила регуляризации\n    'kernel': ['linear', 'rbf'],          # Тип ядра\n    'gamma': ['scale', 'auto']            # Коэффициент ядра\n}\n\nsvc_grid = GridSearchCV(\n    SVC(random_state=42, probability=True),\n    param_grid_svc,\n    cv=3,  # Меньше фолдов для ускорения\n    scoring='roc_auc',\n    n_jobs=-1,\n    verbose=1\n)\n\nprint(\"Подбор гиперпараметров для SVC...\")\nprint(\"(это может занять несколько минут...)\")\nsvc_grid.fit(X_train_combined, y_train)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"РЕЗУЛЬТАТЫ ПОДБОРА ГИПЕРПАРАМЕТРОВ - SVC\")\nprint(\"=\"*60)\nprint(f\"Лучшие параметры: {svc_grid.best_params_}\")\nprint(f\"Лучший ROC-AUC (CV): {svc_grid.best_score_:.4f}\")\n\n# Тестируем на тестовой выборке\ny_proba_svc_tuned = svc_grid.best_estimator_.predict_proba(X_test_combined)[:, 1]\nroc_auc_svc_tuned = roc_auc_score(y_test, y_proba_svc_tuned)\nprint(f\"ROC-AUC на тесте: {roc_auc_svc_tuned:.4f}\")\nprint(f\"Улучшение по сравнению с baseline: {roc_auc_svc_tuned - roc_auc_svc:+.4f}\")\nprint(\"=\"*60)\n\nprint(\"\\nОПИСАНИЕ ГИПЕРПАРАМЕТРОВ:\")\nprint(\"1. C - параметр регуляризации:\")\nprint(\"   - Меньшие значения (0.1) = широкая разделяющая полоса = проще модель\")\nprint(\"   - Большие значения (10) = узкая разделяющая полоса = сложнее модель\")\nprint(\"\\n2. kernel - тип ядра:\")\nprint(\"   - 'linear': линейное разделение, быстрое обучение\")\nprint(\"   - 'rbf': нелинейное разделение (радиальная базисная функция)\")\nprint(\"\\n3. gamma - коэффициент ядра (для rbf):\")\nprint(\"   - 'scale': автоматический расчет на основе данных (рекомендуется)\")\nprint(\"   - 'auto': 1 / n_features\")\nprint(\"   - Большие gamma = учитывается только ближайшее окружение точки\")",
   "metadata": {
    "id": "q8TY3Js6dchY"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "**ОБЩИЕ ВЫВОДЫ ПО ПОДБОРУ ГИПЕРПАРАМЕТРОВ:**\n\n1. Подбор гиперпараметров позволяет значительно улучшить качество моделей\n2. LogisticRegression обучается быстрее и хорошо подходит для текстовых данных\n3. SVC может показывать лучшее качество, но требует больше времени на обучение\n4. Использование GridSearchCV с кросс-валидацией помогает избежать переобучения\n5. Важно балансировать между сложностью модели и обобщающей способностью через регуляризацию",
   "metadata": {
    "id": "NzpJ0zP_db19"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ваши выводы здесь."
   ],
   "metadata": {
    "id": "WyBkbucodifU"
   }
  },
  {
   "cell_type": "code",
   "source": "from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import FunctionTransformer\n\n# Создаем функции для извлечения признаков\ndef get_text(df):\n    \"\"\"Извлекает текстовый столбец\"\"\"\n    if isinstance(df, pd.Series):\n        return df\n    return df['text'] if 'text' in df.columns else df\n\ndef get_keyword(df):\n    \"\"\"Извлекает keyword столбец\"\"\"\n    if isinstance(df, pd.Series):\n        return pd.DataFrame({'keyword': [''] * len(df)})\n    return df[['keyword']].fillna('')\n\ndef get_location(df):\n    \"\"\"Извлекает location столбец\"\"\"\n    if isinstance(df, pd.Series):\n        return pd.DataFrame({'location': [''] * len(df)})\n    return df[['location']].fillna('')\n\n# Пайплайн для LogisticRegression\npipeline_lr = Pipeline([\n    ('features', FeatureUnion([\n        ('text', Pipeline([\n            ('selector', FunctionTransformer(lambda x: x if isinstance(x, pd.Series) else x['text'].fillna(''), validate=False)),\n            ('vectorizer', CountVectorizer(max_features=1200, min_df=2, max_df=0.8, ngram_range=(1, 2)))\n        ])),\n        ('keyword', Pipeline([\n            ('selector', FunctionTransformer(lambda x: x[['keyword']].fillna('') if isinstance(x, pd.DataFrame) else pd.DataFrame({'keyword': ['']*len(x)}), validate=False)),\n            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n        ])),\n        ('location', Pipeline([\n            ('selector', FunctionTransformer(lambda x: x[['location']].fillna('') if isinstance(x, pd.DataFrame) else pd.DataFrame({'location': ['']*len(x)}), validate=False)),\n            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n        ]))\n    ])),\n    ('classifier', LogisticRegression(C=10, max_iter=1000, random_state=42))\n])\n\n# Пайплайн для SVC\npipeline_svc = Pipeline([\n    ('features', FeatureUnion([\n        ('text', Pipeline([\n            ('selector', FunctionTransformer(lambda x: x if isinstance(x, pd.Series) else x['text'].fillna(''), validate=False)),\n            ('vectorizer', CountVectorizer(max_features=1200, min_df=2, max_df=0.8, ngram_range=(1, 2)))\n        ])),\n        ('keyword', Pipeline([\n            ('selector', FunctionTransformer(lambda x: x[['keyword']].fillna('') if isinstance(x, pd.DataFrame) else pd.DataFrame({'keyword': ['']*len(x)}), validate=False)),\n            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n        ])),\n        ('location', Pipeline([\n            ('selector', FunctionTransformer(lambda x: x[['location']].fillna('') if isinstance(x, pd.DataFrame) else pd.DataFrame({'location': ['']*len(x)}), validate=False)),\n            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n        ]))\n    ])),\n    ('classifier', SVC(C=10, kernel='rbf', gamma='scale', probability=True, random_state=42))\n])\n\nprint(\"=\"*60)\nprint(\"СОЗДАНИЕ И ТЕСТИРОВАНИЕ ПАЙПЛАЙНОВ\")\nprint(\"=\"*60)\n\n# Подготовка данных для пайплайнов (используем исходный DataFrame)\nX_train_df = data.loc[X_train.index, ['text', 'keyword', 'location']]\nX_test_df = data.loc[X_test.index, ['text', 'keyword', 'location']]\n\n# Обучение и тестирование пайплайна LogisticRegression\nprint(\"\\nОбучение пайплайна LogisticRegression...\")\npipeline_lr.fit(X_train_df, y_train)\ny_proba_pipeline_lr = pipeline_lr.predict_proba(X_test_df)[:, 1]\nroc_auc_pipeline_lr = roc_auc_score(y_test, y_proba_pipeline_lr)\n\nprint(f\"ROC-AUC (LogisticRegression Pipeline): {roc_auc_pipeline_lr:.4f}\")\n\n# Обучение и тестирование пайплайна SVC\nprint(\"\\nОбучение пайплайна SVC...\")\npipeline_svc.fit(X_train_df, y_train)\ny_proba_pipeline_svc = pipeline_svc.predict_proba(X_test_df)[:, 1]\nroc_auc_pipeline_svc = roc_auc_score(y_test, y_proba_pipeline_svc)\n\nprint(f\"ROC-AUC (SVC Pipeline): {roc_auc_pipeline_svc:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"СТРУКТУРА ПАЙПЛАЙНОВ:\")\nprint(\"=\"*60)\nprint(\"\\nОба пайплайна включают:\")\nprint(\"1. Feature extraction:\")\nprint(\"   - Text: CountVectorizer с оптимизированными параметрами\")\nprint(\"   - Keyword: OneHotEncoder для категориальных признаков\")\nprint(\"   - Location: OneHotEncoder для категориальных признаков\")\nprint(\"2. Feature union: объединение всех признаков\")\nprint(\"3. Classifier: LogisticRegression или SVC с настроенными параметрами\")\nprint(\"\\nПреимущества пайплайна:\")\nprint(\"- Автоматическая предобработка новых данных\")\nprint(\"- Предотвращение утечки данных между train и test\")\nprint(\"- Удобство в продакшене\")\nprint(\"- Простота в использовании с GridSearchCV\")\nprint(\"=\"*60)",
   "metadata": {
    "id": "ix0MxY8Td2uv"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Ваш код здесь"
   ],
   "metadata": {
    "id": "3JOk1suJrTGp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Опишите общие мысли о работе. Это место для вашей рефлексии, не обязательное, но полезное.**  🐤"
   ],
   "metadata": {
    "id": "GjIyXgwGrTXd"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ttkyYHcHrbyb"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "fxmJha91VB90"
   ],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
